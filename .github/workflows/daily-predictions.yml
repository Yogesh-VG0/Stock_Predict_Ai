name: Daily ML Predictions

on:
  schedule:
    # Mon-Fri. Use a buffer after close (covers EST/EDT better than 21:30 UTC)
    - cron: "15 22 * * 1-5"
  workflow_dispatch:

concurrency:
  group: daily-ml-predictions
  cancel-in-progress: false

env:
  MONGODB_URI: ${{ secrets.MONGODB_URI }}
  SKIP_SEC_FMP: "true"

jobs:
  update-predictions:
    runs-on: ubuntu-latest
    timeout-minutes: 120

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"
          cache-dependency-path: ml_backend/requirements-prod.txt

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r ml_backend/requirements-prod.txt

      - name: Run sentiment cron (populate sentiment data)
        env:
          MONGODB_URI: ${{ secrets.MONGODB_URI }}
          FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
          FMP_API_KEY: ${{ secrets.FMP_API_KEY }}
          MARKETAUX_API_KEY: ${{ secrets.MARKETAUX_API_KEY }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}
          REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
          REDDIT_USER_AGENT: "StockPredictAI/1.0"
          PYTHONPATH: ${{ github.workspace }}
        run: |
          echo "===== Running sentiment cron ====="
          python -m ml_backend.sentiment_cron || echo "Sentiment cron failed (non-fatal)"

      - name: Train pooled model on all 100 tickers
        env:
          MONGODB_URI: ${{ secrets.MONGODB_URI }}
          FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
          FRED_API_KEY: ${{ secrets.FRED_API_KEY }}
          FMP_API_KEY: ${{ secrets.FMP_API_KEY }}
          ALPHAVANTAGE_API_KEY: ${{ secrets.ALPHAVANTAGE_API_KEY }}
          MARKETAUX_API_KEY: ${{ secrets.MARKETAUX_API_KEY }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}
          REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
          REDDIT_USER_AGENT: "StockPredictAI/1.0"
          PYTHONPATH: ${{ github.workspace }}
          GITHUB_SHA: ${{ github.sha }}
          RUN_ID: ${{ github.run_id }}
        run: |
          set -e
          echo "===== Training pooled model on ALL 100 tickers ====="
          # Train on the full S&P 100 cross-section so the pooled model
          # sees all tickers.  --no-predict skips storing predictions
          # (handled in the batch loop below).
          python -m ml_backend.scripts.run_pipeline --all-tickers --no-predict

      - name: Generate predictions (sequential batches)
        env:
          MONGODB_URI: ${{ secrets.MONGODB_URI }}
          FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
          FRED_API_KEY: ${{ secrets.FRED_API_KEY }}
          FMP_API_KEY: ${{ secrets.FMP_API_KEY }}
          ALPHAVANTAGE_API_KEY: ${{ secrets.ALPHAVANTAGE_API_KEY }}
          MARKETAUX_API_KEY: ${{ secrets.MARKETAUX_API_KEY }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}
          REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
          REDDIT_USER_AGENT: "StockPredictAI/1.0"
          PYTHONPATH: ${{ github.workspace }}
          GITHUB_SHA: ${{ github.sha }}
          RUN_ID: ${{ github.run_id }}
        run: |
          set -e

          # 10 batches × 10 tickers = 100 stocks
          # Models are already trained above; --predict-only loads them from disk.
          BATCHES=(
            "AAPL MSFT NVDA GOOGL META AVGO ORCL CRM AMD INTC"
            "CSCO ADBE QCOM TXN NOW INTU AMZN TSLA HD NFLX"
            "LOW SBUX NKE MCD DIS BKNG TGT JPM V MA"
            "BAC WFC GS MS AXP BLK SCHW C COF BK"
            "MET AIG USB XOM CVX COP JNJ UNH LLY PFE"
            "ABBV ABT TMO DHR MRK AMGN GILD ISRG MDT BMY"
            "CVS WMT COST PG KO PEP MDLZ CL MO CAT"
            "HON UNP BA RTX LMT DE GE GD EMR FDX"
            "UPS MMM CMCSA VZ T CHTR BRK-B ACN IBM PYPL"
            "LIN NEE SO DUK AMT SPG PLTR TMUS PM AMAT"
          )

          FAILED_BATCHES=()
          for idx in "${!BATCHES[@]}"; do
            BATCH="${BATCHES[$idx]}"
            BATCH_NUM=$((idx + 1))
            echo ""
            echo "===== Batch $BATCH_NUM/${#BATCHES[@]}: $BATCH ====="

            SUCCESS=0
            for attempt in 1 2 3; do
              echo "  Attempt $attempt..."
              if python -m ml_backend.scripts.run_pipeline --predict-only --tickers $BATCH 2>&1 | tee "batch_${BATCH_NUM}_attempt_${attempt}.log"; then
                SUCCESS=1
                break
              fi
              echo "  Attempt $attempt failed. Last error:"
              tail -50 "batch_${BATCH_NUM}_attempt_${attempt}.log" || echo "  (log file not found)"
              sleep $((attempt * 20))
            done

            if [ "$SUCCESS" -eq 0 ]; then
              echo "  *** Batch $BATCH_NUM FAILED after 3 attempts ***"
              FAILED_BATCHES+=("$BATCH_NUM")
            fi

            # Rate limit: pause between batches (API courtesy — paces Finnhub 60/min)
            if [ "$BATCH_NUM" -lt "${#BATCHES[@]}" ]; then
              sleep 10
            fi
          done

          echo ""
          echo "===== Pipeline Summary ====="
          echo "Total batches: ${#BATCHES[@]}"
          echo "Failed batches: ${#FAILED_BATCHES[@]}"
          if [ "${#FAILED_BATCHES[@]}" -gt 0 ]; then
            echo "Failed batch numbers: ${FAILED_BATCHES[*]}"
            # Fail the job if more than 30% of batches failed
            if [ "${#FAILED_BATCHES[@]}" -gt 3 ]; then
              echo "Too many failures — failing job."
              exit 1
            fi
            echo "Partial success — continuing."
          fi

      - name: Verify predictions are fresh
        env:
          MONGODB_URI: ${{ secrets.MONGODB_URI }}
        run: |
          python - << 'PY'
          import os
          from datetime import datetime, timedelta
          from pymongo import MongoClient

          uri = os.getenv("MONGODB_URI")
          if not uri:
            raise SystemExit("Missing MONGODB_URI")

          client = MongoClient(uri, serverSelectionTimeoutMS=8000, connectTimeoutMS=8000)
          client.admin.command("ping")
          db = client["stock_predictor"]
          col = db["stock_predictions"]

          # Use naive UTC (matches MongoDB's naive datetime storage)
          now = datetime.utcnow()
          cutoff = now - timedelta(hours=3)

          horizons = ["next_day", "7_day", "30_day"]

          # Check a representative sample (canary tickers from each batch)
          canary = ["AAPL", "AMZN", "JPM", "XOM", "WMT", "CAT", "CMCSA", "PLTR"]
          missing = []
          stale = []

          for t in canary:
            for w in horizons:
              q = {"ticker": t, "window": {"$eq": w}}
              doc = col.find_one(q, sort=[("timestamp", -1)])
              if not doc:
                missing.append((t, w))
                continue
              ts = doc.get("timestamp")
              if not ts or ts < cutoff:
                stale.append((t, w, ts))

          # Also check total doc count to ensure scale
          total = col.count_documents({"timestamp": {"$gte": cutoff}})
          print(f"Fresh docs in last 3h: {total}")
          print(f"Canary check: {len(canary)} tickers x {len(horizons)} horizons")
          if missing:
            print("Missing:", missing)
          if stale:
            print("Stale:", stale)

          assert not missing, f"Missing predictions: {missing}"
          assert not stale, f"Stale predictions: {stale}"
          assert total >= 200, f"Expected >= 200 fresh docs, got {total}"
          print("Freshness verified")
          PY

      - name: Generate SHAP feature importance
        env:
          MONGODB_URI: ${{ secrets.MONGODB_URI }}
          PYTHONPATH: ${{ github.workspace }}
        run: |
          echo "===== Running SHAP analysis for all 100 tickers ====="
          # Process in batches of 10 to avoid timeouts
          TICKER_BATCHES=(
            "AAPL MSFT NVDA GOOGL META AVGO ORCL CRM AMD INTC"
            "CSCO ADBE QCOM TXN NOW INTU AMZN TSLA HD NFLX"
            "LOW SBUX NKE MCD DIS BKNG TGT JPM V MA"
            "BAC WFC GS MS AXP BLK SCHW C COF BK"
            "MET AIG USB XOM CVX COP JNJ UNH LLY PFE"
            "ABBV ABT TMO DHR MRK AMGN GILD ISRG MDT BMY"
            "CVS WMT COST PG KO PEP MDLZ CL MO CAT"
            "HON UNP BA RTX LMT DE GE GD EMR FDX"
            "UPS MMM CMCSA VZ T CHTR BRK-B ACN IBM PYPL"
            "LIN NEE SO DUK AMT SPG PLTR TMUS PM AMAT"
          )
          SHAP_FAILED=0
          for batch in "${TICKER_BATCHES[@]}"; do
            python -m ml_backend.explain.shap_analysis --tickers $batch || {
              echo "SHAP batch failed: $batch"
              SHAP_FAILED=$((SHAP_FAILED + 1))
            }
          done
          echo "SHAP batches failed: $SHAP_FAILED / ${#TICKER_BATCHES[@]}"
          if [ "$SHAP_FAILED" -gt 5 ]; then
            echo "Too many SHAP failures"
            exit 1
          fi

      - name: Generate AI explanations (Groq/Gemini)
        env:
          MONGODB_URI: ${{ secrets.MONGODB_URI }}
          FRED_API_KEY: ${{ secrets.FRED_API_KEY }}
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          PYTHONPATH: ${{ github.workspace }}
        run: |
          echo "===== Generating AI explanations (Groq preferred, Gemini fallback) ====="
          python -m ml_backend.scripts.generate_explanations || echo "Explanation generation failed (non-fatal)"

      - name: Evaluate stored predictions (last 60 days)
        env:
          MONGODB_URI: ${{ secrets.MONGODB_URI }}
          PYTHONPATH: ${{ github.workspace }}
        run: |
          echo "===== Running evaluate_models --stored ====="
          python -m ml_backend.scripts.evaluate_models --stored --days 60 \
            | tee eval_report.txt || echo "Evaluation failed (non-fatal)"

      - name: Run drift monitor
        env:
          MONGODB_URI: ${{ secrets.MONGODB_URI }}
          PYTHONPATH: ${{ github.workspace }}
        run: |
          echo "===== Running drift_monitor ====="
          python -m ml_backend.scripts.drift_monitor \
            | tee drift_report.txt || echo "Drift monitor failed (non-fatal)"

      - name: Data retention cleanup (remove data older than 12 months)
        env:
          MONGODB_URI: ${{ secrets.MONGODB_URI }}
          PYTHONPATH: ${{ github.workspace }}
        run: |
          echo "===== Running data retention cleanup ====="
          python -m ml_backend.scripts.data_retention \
            | tee retention_report.txt || echo "Retention cleanup failed (non-fatal)"

      - name: Upload logs/artifacts (always)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-logs-${{ github.run_id }}
          path: |
            ml_backend/**/*.log
            **/*.log
            batch_*_attempt_*.log
            eval_report.txt
            drift_report.txt
            retention_report.txt
          if-no-files-found: ignore
